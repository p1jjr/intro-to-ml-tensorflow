{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nVxryO0s_uVp"
   },
   "source": [
    "# Loading Image Data\n",
    "\n",
    "In the previous lessons, we've used TensorFlow Datasets to load our data. However, there might be times, where we already have our dataset saved on a local disk. In this notebook we will learn how to load a dataset from our local disk and use the `tf.keras.preprocessing.image.ImageDataGenerator` class to generate batches of images and perform real-time image augmentation.\n",
    "\n",
    "Also, in the previous notebooks, we've been working with fairly artificial image datasets that we probably wouldn't use in real projects. In practice, we'll likely be dealing with full-sized color images like the ones we get from smart phone cameras. In this notebook we'll be using a filtered version of the [Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats) dataset available from Kaggle. Here are a couple example images:\n",
    "\n",
    "<img src='assets/dog_cat.png'>\n",
    "\n",
    "We'll use this dataset to train a neural network that can differentiate between cats and dogs. These days it doesn't seem like a big accomplishment, but some years ago it was a serious challenge for computer vision systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KlNYoJi1Rh5Z"
   },
   "source": [
    "## Import Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3yWysDPCnCM"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52156,
     "status": "ok",
     "timestamp": 1568565102011,
     "user": {
      "displayName": "Juan Delgado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDy2JEW1zIU-3rz84z5eQq7vBrp-2QY7sMq--L=s64",
      "userId": "11913820249708469300"
     },
     "user_tz": 420
    },
    "id": "4-cSiu8wuDcB",
    "outputId": "59b284a8-176f-450c-97d1-e50da9569ec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:\n",
      "\t• TensorFlow version: 2.9.2\n",
      "\t• tf.keras version: 2.9.0\n",
      "\t• Running on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-24 08:15:16.195735: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-24 08:15:16.195773: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "print('Using:')\n",
    "print('\\t\\u2022 TensorFlow version:', tf.__version__)\n",
    "print('\\t\\u2022 tf.keras version:', tf.keras.__version__)\n",
    "print('\\t\\u2022 Running on GPU' if tf.test.is_gpu_available() else '\\t\\u2022 GPU device not found. Running on CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BC4zwRRAaOb"
   },
   "source": [
    "## Load the Dataset\n",
    "\n",
    "We will now download the filtered Dogs vs. Cats dataset to our local disk. To do this, we are going to use the function:\n",
    "\n",
    "```python\n",
    "tf.keras.utils.get_file(fname, origin)\n",
    "```\n",
    "\n",
    "This function downloads a file from the given `origin` and returns a path to the downloaded file. By default, the file is downloaded to the cache directory `~/.keras/datasets/` and given the filename `fname`. For example, if we used `fname = sample.txt` then the above function will return the following path to the downloaded file:\n",
    "\n",
    "```\n",
    "~/.keras/datasets/sample.txt\n",
    "```\n",
    "\n",
    "where `~/` refers to the current user's home folder. The `origin` parameter is a string containing the original URL of the file.\n",
    "\n",
    "In the cell below we will use the `tf.keras.utils.get_file()` function to download the filtered Dogs vs. Cats dataset from a given `URL`. This dataset is contained in a zip file, therefore, we will also include the `extract = True` argument to extract the file as a zip archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 54958,
     "status": "ok",
     "timestamp": 1568565104821,
     "user": {
      "displayName": "Juan Delgado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDy2JEW1zIU-3rz84z5eQq7vBrp-2QY7sMq--L=s64",
      "userId": "11913820249708469300"
     },
     "user_tz": 420
    },
    "id": "yfuPOqli5KQN",
    "outputId": "4b289e7c-1ba3-4160-feb2-aea3858b62e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
      "68606236/68606236 [==============================] - 32s 0us/step\n"
     ]
    }
   ],
   "source": [
    "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "\n",
    "zip_dir = tf.keras.utils.get_file('cats_and_dogs_filterted.zip', origin=_URL, extract=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bagh3mp-_oCt"
   },
   "source": [
    "The dataset we downloaded above has the following directory structure:\n",
    "\n",
    "<pre style=\"font-size: 10.0pt; font-family: Arial; line-height: 2; letter-spacing: 1.0pt;\" >\n",
    "<b>cats_and_dogs_filtered</b>\n",
    "|__ <b>train</b>\n",
    "    |______ <b>cats</b>: [cat.0.jpg, cat.1.jpg, cat.2.jpg ....]\n",
    "    |______ <b>dogs</b>: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...]\n",
    "|__ <b>validation</b>\n",
    "    |______ <b>cats</b>: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ....]\n",
    "    |______ <b>dogs</b>: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]\n",
    "</pre>\n",
    "\n",
    "As we can see, all the data is downloaded to single directory called `cats_and_dogs_filtered`. Within this directory there is a subdirectory called `train` that holds our training data, and subdirectory called `validation` that holds our validation or testing data. So this dataset has already been separated into a training and a validation set. The data has also been further separated such that each class (cat and dog) has its own directory within the training and validation folders. \n",
    "\n",
    "We will now use the `os` module to assign the paths of the training and validation images to some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKqeK-dG_UsZ"
   },
   "outputs": [],
   "source": [
    "base_dir = os.path.join(os.path.dirname(zip_dir), 'cats_and_dogs_filtered')\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')  \n",
    "\n",
    "# directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')  \n",
    "\n",
    "# directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "\n",
    " # directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9fr6BGRAnPz"
   },
   "source": [
    "## Explore the Dataset\n",
    "\n",
    "Now, let's take a look at how many cat and dog images we have in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 822,
     "status": "ok",
     "timestamp": 1568565331187,
     "user": {
      "displayName": "Juan Delgado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDy2JEW1zIU-3rz84z5eQq7vBrp-2QY7sMq--L=s64",
      "userId": "11913820249708469300"
     },
     "user_tz": 420
    },
    "id": "xr_R4m7hAuD1",
    "outputId": "c4f9f038-deaa-4f31-adf4-6599a053c612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains:\n",
      "• 2,000 training images\n",
      "• 1,000 validation images\n",
      "\n",
      "The training set contains:\n",
      "• 1,000 cat images\n",
      "• 1,000 dog images\n",
      "\n",
      "The validation set contains:\n",
      "• 500 cat images\n",
      "• 500 dog images\n"
     ]
    }
   ],
   "source": [
    "num_cats_tr = len(os.listdir(train_cats_dir))\n",
    "num_dogs_tr = len(os.listdir(train_dogs_dir))\n",
    "\n",
    "num_cats_val = len(os.listdir(validation_cats_dir))\n",
    "num_dogs_val = len(os.listdir(validation_dogs_dir))\n",
    "\n",
    "total_train = num_cats_tr + num_dogs_tr\n",
    "total_val = num_cats_val + num_dogs_val\n",
    "\n",
    "print('The dataset contains:')\n",
    "print('\\u2022 {:,} training images'.format(total_train))\n",
    "print('\\u2022 {:,} validation images'.format(total_val))\n",
    "\n",
    "print('\\nThe training set contains:')\n",
    "print('\\u2022 {:,} cat images'.format(num_cats_tr))\n",
    "print('\\u2022 {:,} dog images'.format(num_dogs_tr))\n",
    "\n",
    "print('\\nThe validation set contains:')\n",
    "print('\\u2022 {:,} cat images'.format(num_cats_val))\n",
    "print('\\u2022 {:,} dog images'.format(num_dogs_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot an image from our dataset. In this case, however, in order to plot our images, we need to create a pipeline first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pipeline\n",
    "\n",
    "In order to create a pipeline in this case, we need to create a generator by using the `ImageDataGenerator` class. We will use the `ImageDataGenerator` class to apply various transformations to our images. We will see examples of some transformations in the following sections. \n",
    "\n",
    "Once we have defined our generator, we need to use the `.flow_from_directory(directory)` method to load our images from the given `directory`. We will also use the `.flow_from_directory()` method to specify various parameters such as the batch size, whether to shuffle our images or not, and the desired shape of our images. By default, the `.flow_from_directory()` method uses a batch size of `32` and it resizes all images to be `(256, 256)`. \n",
    "\n",
    "In the cell below, we are going to plot a single image from our training set. To do this, we start by creating a generator using the `ImageDataGenerator` class. We will use the `ImageDataGenerator` class to normalize the pixel values of our images by using the `rescale=1./255` argument. Then, we will use the `.flow_from_directory()` method to load our images from our training directory. We will also use the `batch_size`, `shuffle=True`, `target_size`, and `class_mode='binary'` arguments to specify the parameters of our pipeline. As you can see will use a batch size of `64` and we will resize our images to be `(224,224)`. We also use the `class_mode` argument to indicate the type of label arrays that we want to be returned. In this case we have chosen `class_mode='binary'` which returns 1-Dimensional binary labels. By default `class_mode=None`, which means no labels are returned. Finally, we plot our image using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scipy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/apfeli/Desktop/NAS/university/Coding Stuff/Udacity Nanodegree/Project 2/intro-to-ml-tensorflow/lessons/intro-to-tensorflow/Part_7_Loading_Image_Data_(Exercise).ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apfeli/Desktop/NAS/university/Coding%20Stuff/Udacity%20Nanodegree/Project%202/intro-to-ml-tensorflow/lessons/intro-to-tensorflow/Part_7_Loading_Image_Data_%28Exercise%29.ipynb#ch0000014?line=3'>4</a>\u001b[0m image_gen \u001b[39m=\u001b[39m ImageDataGenerator(rescale\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apfeli/Desktop/NAS/university/Coding%20Stuff/Udacity%20Nanodegree/Project%202/intro-to-ml-tensorflow/lessons/intro-to-tensorflow/Part_7_Loading_Image_Data_%28Exercise%29.ipynb#ch0000014?line=5'>6</a>\u001b[0m one_image \u001b[39m=\u001b[39m image_gen\u001b[39m.\u001b[39mflow_from_directory(directory\u001b[39m=\u001b[39mtrain_dir,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apfeli/Desktop/NAS/university/Coding%20Stuff/Udacity%20Nanodegree/Project%202/intro-to-ml-tensorflow/lessons/intro-to-tensorflow/Part_7_Loading_Image_Data_%28Exercise%29.ipynb#ch0000014?line=6'>7</a>\u001b[0m                                           batch_size\u001b[39m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apfeli/Desktop/NAS/university/Coding%20Stuff/Udacity%20Nanodegree/Project%202/intro-to-ml-tensorflow/lessons/intro-to-tensorflow/Part_7_Loading_Image_Data_%28Exercise%29.ipynb#ch0000014?line=7'>8</a>\u001b[0m                                           shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apfeli/Desktop/NAS/university/Coding%20Stuff/Udacity%20Nanodegree/Project%202/intro-to-ml-tensorflow/lessons/intro-to-tensorflow/Part_7_Loading_Image_Data_%28Exercise%29.ipynb#ch0000014?line=8'>9</a>\u001b[0m                                           target_size\u001b[39m=\u001b[39m(IMG_SHAPE,IMG_SHAPE),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apfeli/Desktop/NAS/university/Coding%20Stuff/Udacity%20Nanodegree/Project%202/intro-to-ml-tensorflow/lessons/intro-to-tensorflow/Part_7_Loading_Image_Data_%28Exercise%29.ipynb#ch0000014?line=9'>10</a>\u001b[0m                                           class_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/apfeli/Desktop/NAS/university/Coding%20Stuff/Udacity%20Nanodegree/Project%202/intro-to-ml-tensorflow/lessons/intro-to-tensorflow/Part_7_Loading_Image_Data_%28Exercise%29.ipynb#ch0000014?line=11'>12</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(one_image[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apfeli/Desktop/NAS/university/Coding%20Stuff/Udacity%20Nanodegree/Project%202/intro-to-ml-tensorflow/lessons/intro-to-tensorflow/Part_7_Loading_Image_Data_%28Exercise%29.ipynb#ch0000014?line=12'>13</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/Environments/Privat/Nanodegree/p2/env/lib/python3.8/site-packages/keras/preprocessing/image.py:110\u001b[0m, in \u001b[0;36mIterator.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_index_array()\n\u001b[1;32m    108\u001b[0m index_array \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_array[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m*\u001b[39m idx:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m*\u001b[39m\n\u001b[1;32m    109\u001b[0m                                (idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)]\n\u001b[0;32m--> 110\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_batches_of_transformed_samples(index_array)\n",
      "File \u001b[0;32m~/Environments/Privat/Nanodegree/p2/env/lib/python3.8/site-packages/keras/preprocessing/image.py:350\u001b[0m, in \u001b[0;36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_data_generator:\n\u001b[1;32m    349\u001b[0m   params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_data_generator\u001b[39m.\u001b[39mget_random_transform(x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> 350\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_data_generator\u001b[39m.\u001b[39;49mapply_transform(x, params)\n\u001b[1;32m    351\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_data_generator\u001b[39m.\u001b[39mstandardize(x)\n\u001b[1;32m    352\u001b[0m batch_x[i] \u001b[39m=\u001b[39m x\n",
      "File \u001b[0;32m~/Environments/Privat/Nanodegree/p2/env/lib/python3.8/site-packages/keras/preprocessing/image.py:1800\u001b[0m, in \u001b[0;36mImageDataGenerator.apply_transform\u001b[0;34m(self, x, transform_parameters)\u001b[0m\n\u001b[1;32m   1797\u001b[0m img_col_axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol_axis \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1798\u001b[0m img_channel_axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannel_axis \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1800\u001b[0m x \u001b[39m=\u001b[39m apply_affine_transform(\n\u001b[1;32m   1801\u001b[0m     x,\n\u001b[1;32m   1802\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mtheta\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[1;32m   1803\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mtx\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[1;32m   1804\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mty\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[1;32m   1805\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mshear\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[1;32m   1806\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mzx\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m1\u001b[39;49m),\n\u001b[1;32m   1807\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mzy\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m1\u001b[39;49m),\n\u001b[1;32m   1808\u001b[0m     row_axis\u001b[39m=\u001b[39;49mimg_row_axis,\n\u001b[1;32m   1809\u001b[0m     col_axis\u001b[39m=\u001b[39;49mimg_col_axis,\n\u001b[1;32m   1810\u001b[0m     channel_axis\u001b[39m=\u001b[39;49mimg_channel_axis,\n\u001b[1;32m   1811\u001b[0m     fill_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfill_mode,\n\u001b[1;32m   1812\u001b[0m     cval\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcval,\n\u001b[1;32m   1813\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation_order)\n\u001b[1;32m   1815\u001b[0m \u001b[39mif\u001b[39;00m transform_parameters\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mchannel_shift_intensity\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m   x \u001b[39m=\u001b[39m apply_channel_shift(x,\n\u001b[1;32m   1817\u001b[0m                           transform_parameters[\u001b[39m'\u001b[39m\u001b[39mchannel_shift_intensity\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m   1818\u001b[0m                           img_channel_axis)\n",
      "File \u001b[0;32m~/Environments/Privat/Nanodegree/p2/env/lib/python3.8/site-packages/keras/preprocessing/image.py:2244\u001b[0m, in \u001b[0;36mapply_affine_transform\u001b[0;34m(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mkeras.preprocessing.image.apply_affine_transform\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   2213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_affine_transform\u001b[39m(x, theta\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, tx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, ty\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, shear\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, zx\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, zy\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   2214\u001b[0m                            row_axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, col_axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, channel_axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m   2215\u001b[0m                            fill_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m'\u001b[39m, cval\u001b[39m=\u001b[39m\u001b[39m0.\u001b[39m, order\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   2216\u001b[0m   \u001b[39m\"\"\"Applies an affine transformation specified by the parameters given.\u001b[39;00m\n\u001b[1;32m   2217\u001b[0m \n\u001b[1;32m   2218\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m \u001b[39m      ImportError: if SciPy is not available.\u001b[39;00m\n\u001b[1;32m   2243\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2244\u001b[0m   \u001b[39mif\u001b[39;00m scipy \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2245\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mImage transformations require SciPy. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   2246\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mInstall SciPy.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   2248\u001b[0m   \u001b[39m# Input sanity checks:\u001b[39;00m\n\u001b[1;32m   2249\u001b[0m   \u001b[39m# 1. x must 2D image with one or more channels (i.e., a 3D tensor)\u001b[39;00m\n\u001b[1;32m   2250\u001b[0m   \u001b[39m# 2. channels must be either first or last dimension\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scipy' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "IMG_SHAPE  = 224\n",
    "\n",
    "image_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "one_image = image_gen.flow_from_directory(directory=train_dir,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,\n",
    "                                          target_size=(IMG_SHAPE,IMG_SHAPE),\n",
    "                                          class_mode='binary')\n",
    "\n",
    "plt.imshow(one_image[0][0][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EI5M8zosBDZE"
   },
   "source": [
    "# Data Augmentation\n",
    "\n",
    "A common strategy for training neural networks is to introduce randomness in the input data itself. For example, you can randomly rotate, flip, scale, and/or crop the images in your training set. This will help your network generalize as it's seeing the same images but in different locations, with different sizes, in different orientations, etc.\n",
    "\n",
    "We can implement these transformations by passing them as arguments to the `ImageDataGenerator` class. Let's see some examples. For convenience, let's first define a plotting function that we can use to see the type of transformation that has been applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WbWsbueCA_mO"
   },
   "outputs": [],
   "source": [
    "# This function will plot images in the form of a grid with 1 row and 5 columns\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AyJQO-BlCF1Y"
   },
   "source": [
    "### Flipping Images Horizontally\n",
    "\n",
    "Let's begin by randomly applying a horizontal flip to the images in our training set. We can do this by using the `horizontal_flip=True` argument in the `ImageDataGenerator` class. Notice that we also normalize our pixel values as we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 56160,
     "status": "ok",
     "timestamp": 1568565106054,
     "user": {
      "displayName": "Juan Delgado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDy2JEW1zIU-3rz84z5eQq7vBrp-2QY7sMq--L=s64",
      "userId": "11913820249708469300"
     },
     "user_tz": 420
    },
    "id": "vW2hLxZ8CLqU",
    "outputId": "55bb41f5-a4af-4f5d-de79-2e97e9b4eacf"
   },
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n",
    "\n",
    "train_data_gen = image_gen.flow_from_directory(directory=train_dir,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True, \n",
    "                                               target_size=(IMG_SHAPE,IMG_SHAPE),\n",
    "                                               class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pG-Wk9IECgZA"
   },
   "source": [
    "Now, let's use the plotting function we defined above to see how an individual image will look after this transformation. The transformation will be randomly applied (or not) to our image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60713,
     "status": "ok",
     "timestamp": 1568565110614,
     "user": {
      "displayName": "Juan Delgado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDy2JEW1zIU-3rz84z5eQq7vBrp-2QY7sMq--L=s64",
      "userId": "11913820249708469300"
     },
     "user_tz": 420
    },
    "id": "RrKGd_jjVrW7",
    "outputId": "5657ab8a-92b9-4d30-f1ab-ccbd8129c357"
   },
   "outputs": [],
   "source": [
    "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7n9xcqCVrXB"
   },
   "source": [
    "### Rotating the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXnwkzFuVrXB"
   },
   "source": [
    "Now, let's randomly rotate our images up to a specified number of degrees. We can do this by using the `rotation_range=angle` argument in the `ImageDataGenerator` class. Here, we'll set the maximum angle to be `45` degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60696,
     "status": "ok",
     "timestamp": 1568565110615,
     "user": {
      "displayName": "Juan Delgado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDy2JEW1zIU-3rz84z5eQq7vBrp-2QY7sMq--L=s64",
      "userId": "11913820249708469300"
     },
     "user_tz": 420
    },
    "id": "1zip35pDVrXB",
    "outputId": "f16a2163-1877-4264-8fd7-a9f247073cb5"
   },
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(rescale=1./255, rotation_range=45)\n",
    "\n",
    "train_data_gen = image_gen.flow_from_directory(directory=train_dir,\n",
    "                                               batch_size=BATCH_SIZE, \n",
    "                                               shuffle=True, \n",
    "                                               target_size=(IMG_SHAPE, IMG_SHAPE),\n",
    "                                               class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 71125,
     "status": "ok",
     "timestamp": 1568565121051,
     "user": {
      "displayName": "Juan Delgado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDy2JEW1zIU-3rz84z5eQq7vBrp-2QY7sMq--L=s64",
      "userId": "11913820249708469300"
     },
     "user_tz": 420
    },
    "id": "kVoWh4OIVrXD",
    "outputId": "0ee5b11c-a688-4f7f-ee91-ae95d61552be"
   },
   "outputs": [],
   "source": [
    "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FOqGPL76VrXM"
   },
   "source": [
    "### Applying Zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvqXaD8BVrXN"
   },
   "source": [
    "Now, let's randomly apply Zoom to our images. We can do this by using the `zoom_range=range` argument in the `ImageDataGenerator` class. Here, we'll set the zoom range to be `50%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 71116,
     "status": "ok",
     "timestamp": 1568565121052,
     "user": {
      "displayName": "Juan Delgado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDy2JEW1zIU-3rz84z5eQq7vBrp-2QY7sMq--L=s64",
      "userId": "11913820249708469300"
     },
     "user_tz": 420
    },
    "id": "tGNKLa_YVrXR",
    "outputId": "abc58e45-6c3d-49e3-99a4-3f318a6acc11"
   },
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(rescale=1./255, zoom_range=0.5)\n",
    "\n",
    "train_data_gen = image_gen.flow_from_directory(directory=train_dir,\n",
    "                                               batch_size=BATCH_SIZE, \n",
    "                                               shuffle=True, \n",
    "                                               target_size=(IMG_SHAPE, IMG_SHAPE),\n",
    "                                               class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79447,
     "status": "ok",
     "timestamp": 1568565129389,
     "user": {
      "displayName": "Juan Delgado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDy2JEW1zIU-3rz84z5eQq7vBrp-2QY7sMq--L=s64",
      "userId": "11913820249708469300"
     },
     "user_tz": 420
    },
    "id": "VOvTs32FVrXU",
    "outputId": "8e9fb415-d409-4e5a-ae11-ed337a2158ad"
   },
   "outputs": [],
   "source": [
    "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "usS13KCNVrXd"
   },
   "source": [
    "### Putting It All Together\n",
    "\n",
    "We can pass more than one image transformation at a time to the `ImageDataGenerator` class. Therefore, we can apply all the above transformations in one go by using:\n",
    "\n",
    "```python\n",
    "image_gen = ImageDataGenerator(rescale=1./255,\n",
    "                               horizontal_flip=True\n",
    "                               rotation_range=45,\n",
    "                               zoom_range=0.5)\n",
    "```\n",
    "\n",
    "`tf.keras` offers many other transformations that we can apply to our images. You can take a look at all the available transformations in the [TensorFlow Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#arguments)\n",
    "\n",
    "> **Exercise**: Now is your turn to perform image augmentation. Take a look at the [TensorFlow Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#arguments) and apply the following transformations to the training set:\n",
    "* rotation_range\n",
    "* width_shift_range\n",
    "* height_shift_range\n",
    "* shear_range\n",
    "* zoom_range\n",
    "* horizontal_flip\n",
    "* fill_mode\n",
    "\n",
    "> You are free to choose any values for the above parameters, along as they are valid. After creating your generator, use the `.flow_from_directory()` method to set the `batch_size`, `shuffle`, `target_size`, and `class_mode` of your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79434,
     "status": "ok",
     "timestamp": 1568565129390,
     "user": {
      "displayName": "Juan Delgado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDy2JEW1zIU-3rz84z5eQq7vBrp-2QY7sMq--L=s64",
      "userId": "11913820249708469300"
     },
     "user_tz": 420
    },
    "id": "gnr2xujaVrXe",
    "outputId": "21db2696-279b-4651-c029-980294e8cd71"
   },
   "outputs": [],
   "source": [
    "## Solution\n",
    "image_gen_train = \n",
    "\n",
    "train_data_gen = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AW-pV5awVrXl"
   },
   "source": [
    "Now, let's use the plotting function we defined above to see how an individual image will look after all these transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 88243,
     "status": "ok",
     "timestamp": 1568565138205,
     "user": {
      "displayName": "Juan Delgado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDy2JEW1zIU-3rz84z5eQq7vBrp-2QY7sMq--L=s64",
      "userId": "11913820249708469300"
     },
     "user_tz": 420
    },
    "id": "z2m68eMhVrXm",
    "outputId": "cacef87d-3968-4105-9104-694bbea7bfae"
   },
   "outputs": [],
   "source": [
    "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HF1OPDC1HFof"
   },
   "source": [
    "Your transformed images should look something like this.\n",
    "\n",
    "<center>Training examples:</center>\n",
    "<img src='assets/train_examples.png' width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2HR-_sZRQAr"
   },
   "source": [
    "### Creating a Validation Data Generator\n",
    "\n",
    "Generally, we only apply data augmentation to our training data. Therefore, for the validation set we only need to normalize the pixel values of our images.\n",
    "\n",
    "However, we still need to specify the `batch_size`, `target_size`, and `class_mode` in the `.flow_from_directory()` method. Remember that there is no need to shuffle the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 88234,
     "status": "ok",
     "timestamp": 1568565138206,
     "user": {
      "displayName": "Juan Delgado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDy2JEW1zIU-3rz84z5eQq7vBrp-2QY7sMq--L=s64",
      "userId": "11913820249708469300"
     },
     "user_tz": 420
    },
    "id": "Bbj2yeIkRc2k",
    "outputId": "a6c1f573-222c-4fb9-f20f-d2117ac6a54f"
   },
   "outputs": [],
   "source": [
    "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_data_gen = image_gen_val.flow_from_directory(directory=validation_dir,\n",
    "                                                 batch_size=BATCH_SIZE,\n",
    "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
    "                                                 class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VfqLvnpmR5DM"
   },
   "source": [
    "# Build the Model\n",
    "\n",
    "Now we are going to try to build a neural network that can classify the images in the dogs vs. cats dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_neurons = [1024, 512, 256, 128, 56, 28, 14]\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape = (IMG_SHAPE, IMG_SHAPE, 3)))\n",
    "\n",
    "for neurons in layer_neurons:\n",
    "    model.add(tf.keras.layers.Dense(neurons, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "            \n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n",
    "We can configure our model for training just as we have done previously by using the `.compile()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1ZwQ0UzQDww"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since our training batches are coming from a generator (*i.e.* the `ImageDataGenerator` class), we have to use the `.fit_generator()` method instead of the standard `.fit()` method we've used in previous notebooks to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 152173,
     "status": "ok",
     "timestamp": 1568565202159,
     "user": {
      "displayName": "Juan Delgado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDy2JEW1zIU-3rz84z5eQq7vBrp-2QY7sMq--L=s64",
      "userId": "11913820249708469300"
     },
     "user_tz": 420
    },
    "id": "GGsa9KzpQZEw",
    "outputId": "cb53bbe9-275c-4f8f-b6c7-b8e7c29c8380"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "history = model.fit_generator(train_data_gen,\n",
    "                              epochs=EPOCHS,\n",
    "                              validation_data=val_data_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO (Optional): Attempt to Build a Network that Can Classify the Dogs vs. Cats  Dataset\n",
    "\n",
    "As we can see above, the accuracy achieved by the end of training is less than 50\\%, which is not very good. So, your task is to try to build a neural network that can classify the images in the Dogs vs. Cats  Dataset with high accuracy.\n",
    "\n",
    "You will notice that this is quite a bit more complicated than what you did before with the MNIST and Fashion-MNIST datasets. To be honest, you probably won't get it to work with a fully-connected network, no matter how deep. These images have three color channels and have a higher resolution than the 28 $\\times$ 28 images of the MNIST and Fashion-MNIST datasets.\n",
    "\n",
    "In the next part, I'll show you how to use a pre-trained network to build a model that can actually solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Part 7 - Loading Image Data (Solution).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "624aa136fc926591d456c75f95c20a18e1880715ece508c3ede2cbb9a8baaa0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
